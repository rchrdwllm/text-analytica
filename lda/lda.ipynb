{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2415a7df",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a578c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pandas in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kagglehub) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kagglehub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kagglehub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kagglehub) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "First 5 rows of the DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>category_code</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>first_author</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cs-9308101v1</td>\n",
       "      <td>Dynamic Backtracking</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>8/1/93</td>\n",
       "      <td>8/1/93</td>\n",
       "      <td>['M. L. Ginsberg']</td>\n",
       "      <td>'M. L. Ginsberg'</td>\n",
       "      <td>Because of their occasional need to return to ...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cs-9308102v1</td>\n",
       "      <td>A Market-Oriented Programming Environment and ...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>8/1/93</td>\n",
       "      <td>8/1/93</td>\n",
       "      <td>['M. P. Wellman']</td>\n",
       "      <td>'M. P. Wellman'</td>\n",
       "      <td>Market price systems constitute a well-underst...</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cs-9309101v1</td>\n",
       "      <td>An Empirical Analysis of Search in GSAT</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>9/1/93</td>\n",
       "      <td>9/1/93</td>\n",
       "      <td>['I. P. Gent', 'T. Walsh']</td>\n",
       "      <td>'I. P. Gent'</td>\n",
       "      <td>We describe an extensive study of search in GS...</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cs-9311101v1</td>\n",
       "      <td>The Difficulties of Learning Logic Programs wi...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>11/1/93</td>\n",
       "      <td>11/1/93</td>\n",
       "      <td>['F. Bergadano', 'D. Gunetti', 'U. Trinchero']</td>\n",
       "      <td>'F. Bergadano'</td>\n",
       "      <td>As real logic programmers normally use cut (!)...</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cs-9311102v1</td>\n",
       "      <td>Software Agents: Completing Patterns and Const...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>11/1/93</td>\n",
       "      <td>11/1/93</td>\n",
       "      <td>['J. C. Schlimmer', 'L. A. Hermens']</td>\n",
       "      <td>'J. C. Schlimmer'</td>\n",
       "      <td>To support the goal of allowing users to recor...</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title  \\\n",
       "0  cs-9308101v1                               Dynamic Backtracking   \n",
       "1  cs-9308102v1  A Market-Oriented Programming Environment and ...   \n",
       "2  cs-9309101v1            An Empirical Analysis of Search in GSAT   \n",
       "3  cs-9311101v1  The Difficulties of Learning Logic Programs wi...   \n",
       "4  cs-9311102v1  Software Agents: Completing Patterns and Const...   \n",
       "\n",
       "                  category category_code published_date updated_date  \\\n",
       "0  Artificial Intelligence         cs.AI         8/1/93       8/1/93   \n",
       "1  Artificial Intelligence         cs.AI         8/1/93       8/1/93   \n",
       "2  Artificial Intelligence         cs.AI         9/1/93       9/1/93   \n",
       "3  Artificial Intelligence         cs.AI        11/1/93      11/1/93   \n",
       "4  Artificial Intelligence         cs.AI        11/1/93      11/1/93   \n",
       "\n",
       "                                          authors       first_author  \\\n",
       "0                              ['M. L. Ginsberg']   'M. L. Ginsberg'   \n",
       "1                               ['M. P. Wellman']    'M. P. Wellman'   \n",
       "2                      ['I. P. Gent', 'T. Walsh']       'I. P. Gent'   \n",
       "3  ['F. Bergadano', 'D. Gunetti', 'U. Trinchero']     'F. Bergadano'   \n",
       "4            ['J. C. Schlimmer', 'L. A. Hermens']  'J. C. Schlimmer'   \n",
       "\n",
       "                                             summary  summary_word_count  \n",
       "0  Because of their occasional need to return to ...                  79  \n",
       "1  Market price systems constitute a well-underst...                 119  \n",
       "2  We describe an extensive study of search in GS...                 167  \n",
       "3  As real logic programmers normally use cut (!)...                 174  \n",
       "4  To support the goal of allowing users to recor...                 187  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Information about the DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 136238 entries, 0 to 136237\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   id                  136238 non-null  object\n",
      " 1   title               136238 non-null  object\n",
      " 2   category            136238 non-null  object\n",
      " 3   category_code       136238 non-null  object\n",
      " 4   published_date      136238 non-null  object\n",
      " 5   updated_date        136238 non-null  object\n",
      " 6   authors             136238 non-null  object\n",
      " 7   first_author        136238 non-null  object\n",
      " 8   summary             136238 non-null  object\n",
      " 9   summary_word_count  136238 non-null  int64 \n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "%pip install kagglehub pandas\n",
    "\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Download the dataset\n",
    "dataset_path = kagglehub.dataset_download(\"sumitm004/arxiv-scientific-research-papers-dataset\")\n",
    "\n",
    "# Assuming the downloaded file is a CSV and finding the filename\n",
    "# List files in the downloaded directory\n",
    "files = os.listdir(dataset_path)\n",
    "csv_file = None\n",
    "for file in files:\n",
    "    if file.endswith('.csv'):\n",
    "        csv_file = file\n",
    "        break\n",
    "\n",
    "if not csv_file: raise \"No CSV file found in the downloaded dataset.\"\n",
    "\n",
    "file_path = os.path.join(dataset_path, csv_file)\n",
    "# Read the dataset into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(\"First 5 rows of the DataFrame:\")\n",
    "display(df.head())\n",
    "\n",
    "# Display information about the DataFrame\n",
    "print(\"\\nInformation about the DataFrame:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0869659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(string: str):\n",
    "    match list(map(int, string.split(\"/\"))):\n",
    "        case [month, day, year] if year >= 50:\n",
    "            return (month, day, 1900 + year)\n",
    "        case [month, day, year] if year <= 50:\n",
    "            return (month, day, 2000 + year)\n",
    "        case v:\n",
    "            print(f\"Unknown format: {v}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d24899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents from group: 2020-2025\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>category_code</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>first_author</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>abs-2002.00429v2</td>\n",
       "      <td>Uncertainty Weighted Causal Graphs</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>2/2/20</td>\n",
       "      <td>2/6/20</td>\n",
       "      <td>['Eduardo C. Garrido-Merchán', 'C. Puente', 'A...</td>\n",
       "      <td>'Eduardo C. Garrido-Merchán'</td>\n",
       "      <td>Causality has traditionally been a scientific ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>abs-2002.00434v2</td>\n",
       "      <td>Integrating Deep Reinforcement Learning with M...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>2/2/20</td>\n",
       "      <td>5/19/20</td>\n",
       "      <td>['Ekim Yurtsever', 'Linda Capito', 'Keith Redm...</td>\n",
       "      <td>'Ekim Yurtsever'</td>\n",
       "      <td>Automated driving in urban settings is challen...</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>abs-2002.00509v2</td>\n",
       "      <td>A Machine Consciousness architecture based on ...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>2/2/20</td>\n",
       "      <td>3/14/20</td>\n",
       "      <td>['Eduardo C. Garrido Merchán', 'Martín Molina']</td>\n",
       "      <td>'Eduardo C. Garrido Merchán'</td>\n",
       "      <td>Recent developments in machine learning have p...</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>abs-2002.01080v4</td>\n",
       "      <td>Bridging the Gap: Providing Post-Hoc Symbolic ...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>2/4/20</td>\n",
       "      <td>3/19/22</td>\n",
       "      <td>['Sarath Sreedharan', 'Utkarsh Soni', 'Mudit V...</td>\n",
       "      <td>'Sarath Sreedharan'</td>\n",
       "      <td>As increasingly complex AI systems are introdu...</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3468</th>\n",
       "      <td>abs-2002.01088v1</td>\n",
       "      <td>Neuro-evolutionary Frameworks for Generalized ...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>2/4/20</td>\n",
       "      <td>2/4/20</td>\n",
       "      <td>['Thommen George Karimpanal']</td>\n",
       "      <td>'Thommen George Karimpanal'</td>\n",
       "      <td>The recent successes of deep learning and deep...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              title  \\\n",
       "3464  abs-2002.00429v2                 Uncertainty Weighted Causal Graphs   \n",
       "3465  abs-2002.00434v2  Integrating Deep Reinforcement Learning with M...   \n",
       "3466  abs-2002.00509v2  A Machine Consciousness architecture based on ...   \n",
       "3467  abs-2002.01080v4  Bridging the Gap: Providing Post-Hoc Symbolic ...   \n",
       "3468  abs-2002.01088v1  Neuro-evolutionary Frameworks for Generalized ...   \n",
       "\n",
       "                     category category_code published_date updated_date  \\\n",
       "3464  Artificial Intelligence         cs.AI         2/2/20       2/6/20   \n",
       "3465  Artificial Intelligence         cs.AI         2/2/20      5/19/20   \n",
       "3466  Artificial Intelligence         cs.AI         2/2/20      3/14/20   \n",
       "3467  Artificial Intelligence         cs.AI         2/4/20      3/19/22   \n",
       "3468  Artificial Intelligence         cs.AI         2/4/20       2/4/20   \n",
       "\n",
       "                                                authors  \\\n",
       "3464  ['Eduardo C. Garrido-Merchán', 'C. Puente', 'A...   \n",
       "3465  ['Ekim Yurtsever', 'Linda Capito', 'Keith Redm...   \n",
       "3466    ['Eduardo C. Garrido Merchán', 'Martín Molina']   \n",
       "3467  ['Sarath Sreedharan', 'Utkarsh Soni', 'Mudit V...   \n",
       "3468                      ['Thommen George Karimpanal']   \n",
       "\n",
       "                      first_author  \\\n",
       "3464  'Eduardo C. Garrido-Merchán'   \n",
       "3465              'Ekim Yurtsever'   \n",
       "3466  'Eduardo C. Garrido Merchán'   \n",
       "3467           'Sarath Sreedharan'   \n",
       "3468   'Thommen George Karimpanal'   \n",
       "\n",
       "                                                summary  summary_word_count  \n",
       "3464  Causality has traditionally been a scientific ...                 111  \n",
       "3465  Automated driving in urban settings is challen...                 181  \n",
       "3466  Recent developments in machine learning have p...                 197  \n",
       "3467  As increasingly complex AI systems are introdu...                 148  \n",
       "3468  The recent successes of deep learning and deep...                 144  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Get all unique years from the dataset\n",
    "all_years = { y for _, _, y in map(parse_date, list(df[\"published_date\"])) if isinstance(y, int) }\n",
    "\n",
    "if not all_years:\n",
    "    print(\"No valid years found in the dataset.\")\n",
    "    grouped_documents = {}\n",
    "else:\n",
    "    # Initialize the dictionary to hold the grouped series of documents\n",
    "    buckets = {\n",
    "        2000 + 2004j,\n",
    "        2005 + 2009j,\n",
    "        2010 + 2014j,\n",
    "        2015 + 2019j,\n",
    "        2020 + 2025j,\n",
    "        1900 + 1999j\n",
    "    }\n",
    "    grouped_series = defaultdict(list)\n",
    "\n",
    "    # Group documents into 5-year buckets for years >= 2000\n",
    "    latest_year = max(all_years)\n",
    "    for index, docu in df.iterrows():\n",
    "        parsed_date = parse_date(docu[\"published_date\"])\n",
    "        if not parsed_date: continue\n",
    "\n",
    "        _, _, year = parsed_date\n",
    "        for bucket_bound in buckets:\n",
    "            if bucket_bound.real <= year <= bucket_bound.imag:\n",
    "                group_name = f\"{int(bucket_bound.real)}-{int(bucket_bound.imag)}\"\n",
    "                grouped_series[group_name].append(docu)\n",
    "\n",
    "    # Convert the lists of documents into DataFrames\n",
    "    grouped_documents = { y: pd.DataFrame(v) for y, v in grouped_series.items() }\n",
    "\n",
    "    # Print the head of the first group to verify\n",
    "    if grouped_documents:\n",
    "        first_group_name = sorted(grouped_documents.keys(), reverse=True)[0]\n",
    "        print(f\"Documents from group: {first_group_name}\")\n",
    "        display(grouped_documents[first_group_name].head())\n",
    "    else:\n",
    "        print(\"No documents were grouped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82acc507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ace68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-alphanumeric characters and tokenize\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4363d0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperopt in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: numpy in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from hyperopt) (2.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from hyperopt) (1.16.1)\n",
      "Requirement already satisfied: six in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from hyperopt) (1.17.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from hyperopt) (3.5)\n",
      "Requirement already satisfied: future in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from hyperopt) (1.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from hyperopt) (4.67.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from hyperopt) (3.1.1)\n",
      "Requirement already satisfied: py4j in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from hyperopt) (0.10.9.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->hyperopt) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\michael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\hyperopt\\atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "%pip install hyperopt\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from gensim.models import LdaMulticore, CoherenceModel\n",
    "from hyperopt import fmin, tpe, hp, Trials, space_eval, STATUS_OK\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "\n",
    "class Box[T]:\n",
    "    def __init__(self, data: T):\n",
    "        self.data = data\n",
    "\n",
    "# -------------------------\n",
    "# Hyperopt search space\n",
    "# -------------------------\n",
    "search_space = {\n",
    "    'num_topics': hp.choice('num_topics', list(range(4, 11))),\n",
    "    'alpha': hp.uniform('alpha', 0.01, 0.99),\n",
    "    'beta': hp.uniform('beta', 0.01, 0.99),\n",
    "}\n",
    "\n",
    "def objective(corpus, dictionary, texts, params, group_name, counter: Box[int]):\n",
    "    try:\n",
    "        num_topics = int(params['num_topics'])\n",
    "        alpha = float(params['alpha'])\n",
    "        beta = float(params['beta'])\n",
    "\n",
    "        passes = 15\n",
    "        if len(corpus) >= 10_000:\n",
    "            passes = 10\n",
    "\n",
    "        lda_model = LdaMulticore(corpus=corpus,\n",
    "                                 id2word=dictionary,\n",
    "                                 num_topics=num_topics,\n",
    "                                 random_state=100,\n",
    "                                 chunksize=100,\n",
    "                                 passes=passes,\n",
    "                                 alpha=alpha,\n",
    "                                 eta=beta)\n",
    "\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                             texts=texts,\n",
    "                                             dictionary=dictionary,\n",
    "                                             coherence='c_v')\n",
    "\n",
    "        coherence = coherence_model_lda.get_coherence()\n",
    "\n",
    "\n",
    "        # Create a directory to save the models\n",
    "        if not os.path.exists(f'lda_models/lda_{group_name}'):\n",
    "            os.makedirs(f'lda_models/lda_{group_name}')\n",
    "\n",
    "        # Create a directory to save the models\n",
    "        model_path = f'lda_models/lda_{group_name}/{counter.data}.pkl'\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(lda_model, f)\n",
    "\n",
    "        counter.data += 1\n",
    "        # fmin minimizes, so return negative coherence to maximize it\n",
    "        return {'loss': -coherence, 'status': STATUS_OK}\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # on failure return a large loss\n",
    "        return {'loss': 1e6, 'status': 'fail'}\n",
    "\n",
    "\n",
    "def create_optimized_model(corpus, dictionary, texts, group_name, max_evals=20):\n",
    "    counter = Box(0)\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=lambda params: objective(corpus, dictionary, texts, params, group_name, counter),\n",
    "                space=search_space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=max_evals,\n",
    "                trials=trials,\n",
    "                early_stop_fn=no_progress_loss(5))\n",
    "    parameters = space_eval(search_space, best)\n",
    "\n",
    "    # Cast final parameters\n",
    "    num_topics = int(parameters['num_topics'])\n",
    "    alpha = float(parameters['alpha'])\n",
    "    beta = float(parameters['beta'])\n",
    "\n",
    "    # Train final model with best params\n",
    "    best_lda = LdaMulticore(corpus=corpus,\n",
    "                            id2word=dictionary,\n",
    "                            num_topics=num_topics,\n",
    "                            random_state=100,\n",
    "                            chunksize=100,\n",
    "                            passes=20,\n",
    "                            alpha=alpha,\n",
    "                            eta=beta)\n",
    "\n",
    "    return best_lda, num_topics, alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4e7a27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gensim) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gensim) (1.16.1)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gensim) (7.4.3)\n",
      "Requirement already satisfied: wrapt in c:\\users\\michael\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim\n",
    "\n",
    "from pandas import DataFrame\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def process_lda(dataframe: DataFrame, group_name: str):\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    # Apply the preprocessing function\n",
    "    df['processed_summary'] = df['summary'].apply(preprocess_text)\n",
    "\n",
    "    # Create a dictionary from the processed_summary column\n",
    "    dictionary = Dictionary(df['processed_summary'])\n",
    "\n",
    "    # Create a corpus (bag-of-words representation)\n",
    "    corpus = [dictionary.doc2bow(text) for text in df['processed_summary']]\n",
    "\n",
    "    # Run hyperopt to get an optimized model\n",
    "    lda_model, num_topics, alpha, beta = create_optimized_model(corpus,\n",
    "                                                                dictionary, df['processed_summary'],\n",
    "                                                                group_name)\n",
    "\n",
    "    # Print the topics learned by the LDA model\n",
    "    print(\"LDA Topics:\")\n",
    "    display(lda_model.print_topics(num_words=5))\n",
    "\n",
    "    return lda_model, num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4596d12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1900-1999', '2000-2004', '2005-2009', '2010-2014', '2015-2019', '2020-2025'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For each group of years, we want to run the LDA for their own.\n",
    "from gensim.models import LdaModel\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Create a directory to save the models\n",
    "if not os.path.exists('lda_models'):\n",
    "    os.makedirs('lda_models')\n",
    "\n",
    "ldas: dict[str, tuple[LdaModel, int]] = {}\n",
    "display(grouped_documents.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b2d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group: 1900-1999\n",
      " 25%|██▌       | 5/20 [03:00<09:01, 36.08s/trial, best loss: -0.29849822223691985]\n",
      "LDA Topics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"model\" + 0.013*\"language\" + 0.008*\"method\" + 0.008*\"learning\" + 0.007*\"problem\"'),\n",
       " (1,\n",
       "  '0.014*\"word\" + 0.011*\"text\" + 0.011*\"method\" + 0.009*\"paper\" + 0.009*\"system\"'),\n",
       " (2,\n",
       "  '0.005*\"system\" + 0.005*\"grammar\" + 0.004*\"language\" + 0.004*\"paper\" + 0.004*\"temporal\"'),\n",
       " (3,\n",
       "  '0.014*\"language\" + 0.012*\"system\" + 0.009*\"speech\" + 0.008*\"processing\" + 0.008*\"paper\"'),\n",
       " (4,\n",
       "  '0.007*\"algorithm\" + 0.006*\"information\" + 0.006*\"grammar\" + 0.004*\"used\" + 0.004*\"problem\"'),\n",
       " (5,\n",
       "  '0.012*\"discourse\" + 0.006*\"paper\" + 0.005*\"structure\" + 0.005*\"interpretation\" + 0.005*\"problem\"'),\n",
       " (6,\n",
       "  '0.014*\"grammar\" + 0.010*\"constraint\" + 0.008*\"approach\" + 0.008*\"theory\" + 0.008*\"paper\"')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing for group: 1900-1999\n",
      "Processing group: 2000-2004\n",
      " 10%|█         | 2/20 [01:20<12:07, 40.40s/trial, best loss: -0.3197621681521153] "
     ]
    }
   ],
   "source": [
    "for year_group, dataframe in grouped_documents.items():\n",
    "    print(f\"Processing group: {year_group}\")\n",
    "    lda_result = process_lda(dataframe, year_group) # lda_result: (LdaModel, int)\n",
    "    ldas[year_group] = lda_result\n",
    "    print(f\"Finished processing for group: {year_group}\")\n",
    "\n",
    "    model_path = f'lda_models/lda_{year_group}.pkl'\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(ldas, f)\n",
    "\n",
    "# Save the entire dictionary of models\n",
    "model_path = 'lda_models/all_lda_models.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(ldas, f)\n",
    "\n",
    "print(f\"All models saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
